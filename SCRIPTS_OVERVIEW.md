# Scripts overview (AI4Good submission)

This document tracks what each script does and whether it is needed to reproduce the main results in the paper.

We will fill in **Purpose** and **Status** after inspecting each file.

**Status legend (to be used later)**
- **core** – required to reproduce main experiments / final model
- **support** – helper utilities used by core scripts
- **legacy** – old / redundant experiments, not needed for reproduction

---

## 1. Top-level `scripts/`

- `scripts/README.md`  
  - Purpose: TBD  
  - Status: TBD

- `scripts/dump_features.py`  
  - Purpose: Extract CLIP image features for a given split (train/val/test) and backbone.  
    It reads a CSV split file (`splits/train.csv`, `val.csv`, or `test.csv`) with columns
    `filepath,label`, loads images from `--data-root`, encodes them with a chosen CLIP
    backbone (`--backbone`, `--pretrained`), L2-normalises the features, and saves them
    as a compressed NumPy archive `features/<backbone>/<split>.npz` containing:
    image features `X`, numeric labels `y`, and relative paths `paths`.  
  - Status: **core** (needed to prepare features for all zero-shot and few-shot experiments)
(TODO: fix small typo in `out_dir = Path(args.save-dir)` → `args.save_dir`.)


- `scripts/dump_features.py.save`  
  - Purpose: Older backup version of `dump_features.py` for CLIP feature extraction.  
    It implements the same basic logic (read split CSV, load images, encode with CLIP,
    save `features/<backbone>/<split>.npz`) but without missing-file handling and the
    current code is truncated/corrupted.  
  - Status: **legacy** (not used; kept only as an old backup and safe to move/delete later)


- `scripts/build_text_prompts.py`  
  - Purpose: Build all per-class text prompt sets used in our CLIP experiments.  
    It reads:
      - a split CSV (default `splits/train.csv`) with image filepaths and labels,
      - an attribute database from BLIP (`data/prompts/blip_attributes.json`),
      - optional `labels.tsv` to map numeric labels to class names,
      - a BLIP caption file (`data/prompts/blip_captions.json`),
    then:
      1. Aggregates BLIP-derived attributes per class (cap color, surface, hymenophore, habitat, etc.).
      2. Generates three prompt sets for each class:
         - **plain**: generic CLIP-style prompts with class name only
           → `class_prompts_plain.json`
         - **attr-only**: prompts enriched with attributes but no lighting
           → `class_prompts_attr.json`
         - **enriched**: prompts with attributes + lighting variants
           → `class_prompts_enriched.json`
      3. Uses the caption cleaner (`src/utils/clean_caption.py`) to build
         **cleaned** prompt sets directly from BLIP captions:
           - `class_prompts_attr_clean.json`
           - `class_prompts_enriched_clean.json`  
    All outputs are saved under `data/prompts/*.json` and are later consumed by the
    zero-shot and few-shot evaluation scripts.  
  - Status: **core** (this is the canonical generator for all prompt JSONs used in our experiments)


- `scripts/blip_caption_bank.py`  
  - Purpose: Generate a BLIP-based caption bank for all mushroom images, including an
    estimated lighting description.  
    It:
      - walks the dataset root (`--root`, e.g. `data/raw`) and collects all image files,
      - runs a BLIP captioning model (`BlipProcessor` + `BlipForConditionalGeneration`)
        with multiple text prefixes (e.g., "a detailed photo of", "close up macro shot of"),
      - estimates a lighting phrase per image from average brightness
        (`in low light` / `in moderate light` / `in bright daylight`),
      - generates `k` captions per image, appends the lighting phrase, and deduplicates them.  
    The output is a JSON file (default `data/prompts/blip_captions.json`) mapping each
    image’s **relative path** to:
      - `captions`: list of BLIP captions with prefixes + lighting
      - `lighting`: the chosen lighting phrase.  
    This file is later consumed by `build_text_prompts.py` to create cleaned, attribute-based
    prompt sets.  
  - Status: **core** (needed to reproduce the BLIP-derived caption and prompt pipeline)


- `scripts/extract_attributes.py`  
  - Purpose: Extract simple mushroom attributes from the BLIP caption bank using
    small hand-crafted lexicons.  
    It reads `data/prompts/blip_captions.json` (generated by `blip_caption_bank.py`)
    and, for each image, normalises the captions and looks for keywords describing:
      - `cap_color` (e.g., brown, white, orange, red, …)
      - `surface` (smooth, scaly, slimy, dry, …)
      - `cap_shape` (cup-shaped, funnel-shaped, flat, convex, …)
      - `habitat` (forest, leaf litter, grass, dead wood, …)
      - `hymenophore` (gills, pores, tubes, spines)
      - `hymen_adj` (pale, crowded, decurrent, etc.).  
    For each attribute field it keeps the **most frequent** value over all captions of an image,
    and writes out `data/prompts/blip_attributes.json`, mapping each image (relative path) to
    `{ "attributes": {…}, "lighting": ... }`.  
    It also prints simple coverage statistics for each attribute type.  
    This attribute file is later aggregated per class by `build_text_prompts.py` to build
    attribute-enriched prompt sets.  
  - Status: **core** (bridge between BLIP captions and attribute-based prompt generation)


- `scripts/generate_clip_prompts.py`  
  - Purpose: Experimental standalone generator for very rich CLIP text prompts based on
    hard-coded species-specific knowledge.  
    It defines a `MushroomPromptGenerator` class with:
      - generic templates about morphology, habitat, growth, etc.,
      - a small built-in knowledge base for a subset of species (Amanita, Boletus,
        Cantharellus, Psilocybe, Ganoderma, Laetiporus, Trametes, Lycoperdon, Armillaria),
      - functions to create basic, descriptive, contextual, and morphological prompts.  
    The script reads a **hard-coded** training CSV from  
    `/zfs/ai4good/datasets/mushroom/train.csv`, generates many prompts per species,
    and writes them to `mushroom_prompts.json` in the repo root.  
    It is not used by the current BLIP/attribute-based prompt pipeline
    (`blip_caption_bank.py` → `extract_attributes.py` → `build_text_prompts.py`).  
  - Status: **legacy** (experimental / unused in final pipeline; kept as reference only)


- `scripts/prompt_utils.py`  
  - Purpose: Small helper for loading per-class prompt lists from a JSON file.  
    It exposes:
      - `DELTA_PROMPTS`: currently a hard-coded absolute path to
        `/home/c/dkorot/AI4GOOD/ai4good-mushroom/data_prompts_label/delta_prompts.json`
        (should be made relative for portability in the final version),
      - `load_prompts(prompt_path, labels)`: given a JSON path and a list of class labels,
        returns a dict `{label -> [prompts...]}`.  
    If `prompt_path` is `None`, it falls back to `DELTA_PROMPTS`.  
    If the file is missing or a particular label is not in the JSON, it returns a default
    prompt `["a photo of {label}"]` for that label.  
  - Status: **support** (utility used by other scripts/notebooks to load prompt sets;
    needs minor cleanup of the absolute path for the submission)


- `scripts/clip_baseline.py`  
  - Purpose: Standalone baseline script to run CLIP zero-shot and few-shot classification
    directly on the mushroom dataset CSVs (without using precomputed features).  
    It:
      - reads a train and validation CSV (`--train_csv`, `--val_csv`) with image paths and labels
        and resolves paths from Kaggle-style locations to the ZFS dataset root,
      - loads a CLIP model via `open_clip` (configurable `--model`, `--pretrained`),
      - builds simple generic text prompts per class (e.g. "a photo of a {species} mushroom"),
      - computes **zero-shot** accuracy on the validation set by comparing image and text
        embeddings,
      - optionally, for `--shots > 0`, selects K images per class from the train CSV,
        extracts their image embeddings, trains a multinomial logistic regression classifier,
        and reports **few-shot** validation accuracy.  
    It also redirects model caches to a ZFS directory (`--cache_dir`) to avoid filling `$HOME`.  
  - Status: **legacy** (useful as an early CLIP baseline / sanity check, but not part of the
    final cached-feature + prompt pipeline; also requires cleanup of hard-coded paths and
    interactive `input()` before being runnable by the TAs)


- `scripts/eval_zero_shot.py`  
  - Purpose: Main evaluation script for **zero-shot CLIP baselines** with different
    prompt sets (plain / attribute-only / enriched), using **precomputed image features**.
    This is one of the central scripts that produces the metrics and confusion matrices
    reported in the paper.
  - Inputs:
      - Dataset + labels: `--data-root`, `--train-csv`, `--val-csv`, `--test-csv`,
        `--labels` (labels.tsv with `id\tname`).
      - CLIP model: `--backbone` (e.g. ViT-B-32), `--pretrained` (e.g. openai).
      - Prompt JSONs: `--prompts_plain`, `--prompts_attr`, `--prompts_enriched`
        (typically those built by `build_text_prompts.py`).
  - What it does:
      - Uses `ensure_features(...)` to load (or lazily compute via
        `scripts/dump_features.py`) cached CLIP image features for each split
        (`train`, `val`, `test`) and backbone.
      - Loads label names from `labels.tsv` and builds **one text embedding per class**
        by mean-pooling CLIP text embeddings over all prompts for that class,
        separately for:
          - plain prompts,
          - attribute-only prompts,
          - enriched prompts.
      - On each selected split (`--splits` argument, typically `val` and/or `test`):
          - computes **random** and **majority-class** baselines,
          - evaluates zero-shot performance for each prompt set:
            - top-1 and top-5 accuracy,
            - balanced accuracy,
            - macro F1,
          - saves a **normalized confusion matrix** plot
            `results/confmat_{split}_{backbone}_{mode}.png`.
      - Aggregates metrics into:
          - a CSV summary `results/metrics_{backbone}.csv`
            (rows: split × {random, majority, zero-shot-plain, zero-shot-attr, zero-shot-enriched}),
          - three JSON files `results/metrics/clip_plain.json`,
            `clip_attr.json`, `clip_enriched.json` for easy comparison and plotting.
  - Status: **core** (this is the main evaluation script for the CLIP + prompt
    baselines reported in the Experiments section; must remain runnable and well
    documented for the final submission).


- `scripts/eval_prompt_sets.py`  
  - Purpose: **Exploratory analysis script** to compare several prompt sets on a
    small 200-image subset of the test set, using the original OpenAI `clip`
    package (ViT-B/32) and a *direct CLIP API* (not `open_clip`).  
    It was mainly used to sanity-check different prompt banks and prompt-pooling
    strategies during development.
  - What it evaluates:
      - Baseline v1 templates (`SHORT_PROMPTS` from `clip_utils.py`).
      - Image-derived prompts: `enhanced_mushroom_prompts.json`.
      - Common-name–enhanced prompts: `enhanced_with_common_names_prompts.json`.
      - Net-new prompts only: `delta_prompts.json`.
      - An explicit **union ensemble**: v1 + image-derived + common-name prompts.
  - Method:
      - Loads a shuffled subset of **200 samples** from the global test CSV
        (`/zfs/ai4good/datasets/mushroom/test.csv`).
      - Encodes images with `clip.load("ViT-B/32")`, normalizing embeddings.
      - For each prompt set and each class:
          - builds **mean label embeddings** (mean over prompts),
          - optionally performs **per-prompt pooling** using a temperature-scaled
            softmax over per-prompt cosine similarities.
      - For each configuration (prompt set × mean vs pooling):
          - predicts the label for each image by maximum similarity,
          - computes simple **top-1 accuracy** on the 200-sample subset.
      - Writes a summary JSON `prompt_sets_report.json` with all accuracies and
        prints them to stdout.
  - Status: **legacy / exploratory**.  
    Useful to understand how different prompt banks behaved and to justify some
    design choices in the paper, but **not part of the main final evaluation
    pipeline** (which uses `scripts/eval_zero_shot.py` with `open_clip`).  
    If space is needed for submission, this can be kept in a “dev/analysis”
    section or clearly marked as supplementary.


- `scripts/eval_blip_knn.py`  
  - Purpose: Evaluate a **few-shot BLIP k-nearest-neighbour (kNN) classifier** using
    BLIP vision embeddings only (no text), as an alternative vision backbone to CLIP.
    It serves as an additional baseline to compare against CLIP zero-shot and our
    prompt-based methods.
  - Inputs:
      - Image data: `--data-root` + CSVs (`--train-csv`, `--val-csv`, `--test-csv`)
        with `filepath` and `label` columns.
      - Label mapping file: `--labels` (`labels.tsv`) to define class order.
      - Hugging Face BLIP vision backbone: `--blip_model`
        (default `Salesforce/blip-itm-base-coco`), optionally cached via `--cache_dir`.
  - Method:
      - Loads BLIP vision encoder via `BlipProcessor` + `BlipModel`.
      - Builds a **small gallery** from the train split:
          - Up to `--gallery_per_class` images per class are collected.
          - Encodes them to **L2-normalized vision embeddings** (optionally using
            CLS or mean pooling).
      - For each evaluation split (`val` / `test`):
          - Encodes all query images with the same BLIP vision encoder.
          - Computes cosine similarity to the gallery:
              - **Strict 1-NN top-1**: label of the single nearest gallery vector.
              - **Aggregated top-k class scores**: sum top-5 neighbour similarities
                per class and take argmax; used for balanced accuracy / F1.
      - Metrics per split:
          - `top1`: strict nearest-neighbour accuracy.
          - `top5`: whether the true class is in the top-5 classes by aggregated
            score.
          - `balanced_acc`: per-class accuracy averaged across classes.
          - `macro_f1`: macro-averaged F1.
  - Outputs:
      - Single JSON summary: `--out_json`
        (default `results/metrics/blip_knn.json`), with structure:
        `{"model": ..., "type": "blip_knn", "gallery_per_class": ..., "pooling": ..., "splits": {val/test → metrics}}`.
  - Status:
      - **Auxiliary baseline** to show how a BLIP vision-only few-shot kNN compares to
        CLIP-based approaches.
      - Not needed for the core CLIP+prompt pipeline, but useful to keep as a
        documented experimental baseline.


- `scripts/eval_blip_itm_zero_shot.py`  
  - Purpose: Evaluate a **BLIP image–text matching (ITM) zero-shot classifier**, optionally
    **reranking a CLIP shortlist** of candidate classes. This tests whether BLIP’s
    cross-modal ITM scores can improve over pure CLIP cosine similarity when using
    our enriched text prompts.
  - Inputs:
      - Image data: `--data-root` + CSVs (`--train-csv`, `--val-csv`, `--test-csv`)
        with `filepath` and `label` columns.
      - Label mapping: `--labels` (`labels.tsv`, defines class ordering).
      - Prompt JSON: `--prompts` (default `data/prompts/class_prompts_enriched.json`),
        listing **per-class textual prompts**. Up to `--max_prompts_per_class` are
        used per class.
  - Method:
      1. **Prompt preparation**
         - Loads per-class prompts from JSON; truncates to `max_prompts_per_class`.
         - Pre-tokenizes all prompts with the BLIP `BlipProcessor` once and keeps
           the token tensors cached per class.
      2. **BLIP ITM scoring**
         - Loads `BlipForImageTextRetrieval` (default `Salesforce/blip-itm-base-coco`)
           on the chosen `--device` and dtype (`fp32/fp16/bf16` with autocast on GPU).
         - For each batch of images and each selected class:
             - Repeats image features across all prompts of that class.
             - Runs BLIP ITM to get **match logits** for image–text pairs.
             - Aggregates prompt scores per class with `--aggregator`:
               - `mean`: average match logit across prompts.
               - `max`: best prompt score per image/class.
      3. **Optional CLIP shortlist (default enabled)**
         - If `--shortlist_engine clip`:
             - Uses `dump_features.py`-style CLIP image features
               (`--clip_backbone`, `--clip_pretrained`) and a CLIP text matrix
               built from prompts to compute CLIP scores for all classes.
             - For each image, selects the **top-`shortlist_topk`** classes as a
               shortlist.
             - BLIP ITM is then computed **only for the union of shortlisted
               classes** in the batch, reducing cost.
         - If `--shortlist_engine none`, BLIP ITM evaluates all classes for all images.
  - Evaluation & metrics:
      - For each requested split (`val` / `test` via `--splits`):
          - Scores all images against all (or shortlisted) classes with BLIP ITM.
          - Predictions are `argmax` over class scores.
          - Computes:
              - `top1` accuracy.
              - `top5` accuracy (true label in top-5 scores).
              - `balanced_acc` (per-class averaged accuracy).
              - `macro_f1` (macro-averaged F1).
  - Outputs:
      - Single JSON summary: `--out_json`
        (default `results/metrics/blip_itm_enriched.json`) with:
        - Global config (`model`, `type="blip_itm"`, `aggregator`, shortlist settings).
        - Per-split metrics in `splits.{val,test}`.
  - Status:
      - **Advanced zero-shot baseline** combining:
        - Enriched, attribute-aware text prompts.
        - CLIP-based class shortlist.
        - BLIP ITM cross-modal scoring to rerank classes.
      - Used to assess whether BLIP ITM improves over pure CLIP zero-shot on
        the mushroom dataset; not required for the main CLIP + linear probe pipeline,
        but important for the experimental comparison section.


- `scripts/make_caption_preview.py`  
  - Purpose: Debug/inspect the **BLIP caption → cleaned attributes** pipeline by
    producing a small CSV with example captions, extracted attribute tokens, and
    inferred lighting. This is used for qualitative sanity checks, not for training.
  - Inputs:
      - `--captions`: BLIP caption bank (`data/prompts/blip_captions.json`),
        as produced by `scripts/blip_caption_bank.py`.
      - Uses the caption cleaner `src.utils.clean_caption.clean` to extract
        attribute tokens and a lighting tag from each caption.
  - What it does:
      1. Iterates over images in the BLIP caption JSON; each key is a relative image
         path like `ClassName/image.jpg`, so the class is the first folder.
      2. Optionally filters to classes whose name contains a substring
         (`--class_contains`).
      3. For each image, takes up to `--per_image_captions` BLIP captions:
         - Concatenates them into a single `raw_caption` field.
         - Runs the cleaner on each caption to collect:
           - `attrs` tokens → aggregated into `kept_tokens` (deduplicated, order-preserving).
           - `lighting` values → majority vote over captions.
      4. Stops after `--limit` rows, if specified.
  - Output:
      - A CSV file (default `results/examples/caption_preview_clean.csv`) with:
          - `image_id` (relative path, e.g. `Amanita_muscaria/xxxx.jpg`)
          - `class` (class name from folder)
          - `raw_caption` (concatenation of the selected BLIP captions)
          - `kept_tokens` (comma-separated cleaned attribute tokens)
          - `lighting` (dominant lighting label from the cleaner)
  - Role in the project:
      - **Diagnostic / qualitative tool** to quickly inspect how BLIP captions are
        cleaned and which attributes/lighting cues are extracted.
      - Supports the narrative that our prompt engineering is grounded in
        interpretable attributes, but is **not required** for reproducing the main
        zero-shot / few-shot results.


- `scripts/make_qualitative.py`  
  - Purpose: Generate a **qualitative grid** figure that shows:
      - the input mushroom image,
      - the CLIP **top-5 class predictions** with **plain prompts**,
      - the CLIP **top-5 class predictions** with **enriched prompts**,
      - plus the “winning prompt” (the single prompt that best matches the image) for each variant.  
    This is used for qualitative analysis and figures, not for training.
  - Inputs:
      - Precomputed CLIP features: `features/<backbone>/<split>.npz` (from `scripts/dump_features.py`), containing:
          - `X`: L2-normalized image features,
          - `y`: integer class IDs,
          - `paths`: relative image paths.
      - `--labels`: label file (typically `labels.tsv`) to map class IDs to class names.
      - Two per-class prompt JSONs:
          - `--prompts_plain` (e.g. `data/prompts/class_prompts_plain.json`)
          - `--prompts_enriched` (e.g. `data/prompts/class_prompts_enriched.json`)
  - What it does:
      1. Loads image features and labels for the chosen split (`val` or `test`).
      2. Loads CLIP and builds **text embeddings** for each class:
          - for plain prompts and enriched prompts,
          - both as a mean class embedding (`[D, K]`) and as per-prompt embeddings (needed to find the “winning prompt”).
      3. Computes logits `X @ T` and zero-shot predictions:
          - gets top-1 and top-5 predictions for **plain** and **enriched**.
      4. Selects a mix of examples:
          - cases where enriched is correct but plain is wrong,
          - cases where both are wrong,
          - cases where both are correct (up to `--n_examples` total).
      5. For each selected image:
          - loads the RGB image from `--data-root` + relative path,
          - computes top-5 classes and scores for plain and enriched,
          - finds the “winning prompt” for the predicted class (prompt with highest cosine similarity to that image).
      6. Plots a grid with 3 columns:
          - Image (with ground-truth label, title colored by enriched correctness),
          - Plain: top-5 predictions and the winning plain prompt,
          - Enriched: top-5 predictions and the winning enriched prompt.
  - Output:
      - A single PNG figure (default `results/examples/qualitative.png`) with one row per example:
          - Column 1: image + GT label,
          - Column 2: text block for **plain** prompts (top-5 + winning prompt),
          - Column 3: text block for **enriched** prompts (top-5 + winning prompt).
  - Role in the project:
      - **Qualitative visualization tool** used to show how attribute-enriched prompts change the prediction distribution
        and which prompts actually “fire” on specific images.
      - Good candidate figure for the **Experiments / Qualitative Results** section, but not required to reproduce metrics.


- `scripts/plot_ablations.py`  
  - Purpose: Plot a **bar chart** comparing zero-shot **top-1 accuracy** for three prompt variants:
      - PLAIN prompts,
      - +ATTR (attribute-only),
      - +ATTR+LIGHT (enriched with attributes and lighting).
    This produces the ablation figure we use in the paper.
  - Inputs:
      - `--plain`: JSON metrics from `eval_zero_shot.py` for plain prompts  
        (default: `results/metrics/clip_plain.json`).
      - `--attr`: JSON metrics for attribute-only prompts  
        (default: `results/metrics/clip_attr.json`).
      - `--enriched`: JSON metrics for attribute+lighting prompts  
        (default: `results/metrics/clip_enriched.json`).
      - Each JSON is expected to have a `"splits"` dict with per-split metrics, including `"top1"`.
      - `--splits`: which splits to plot on the x-axis (default `["val", "test"]`).
  - What it does:
      1. Loads each JSON and extracts the **top-1 accuracy per split**.
      2. For each requested split (e.g. `val`, `test`), builds three bars:
          - PLAIN (baseline),
          - +ATTR,
          - +ATTR+LIGHT.
      3. Creates a grouped bar plot with:
          - x-axis: dataset split(s),
          - y-axis: top-1 accuracy,
          - legend: `PLAIN`, `+ATTR`, `+ATTR+LIGHT`.
  - Output:
      - A PNG figure (default `results/figs/ablations.png`) showing the ablation comparison.
  - Role in the project:
      - **Summarizes the impact of attributes and lighting on zero-shot performance**.
      - Directly used as the ablation figure in the **Experiments** section of the paper.


- `scripts/Lora.py`  
  - Purpose: Fine-tune **OpenAI CLIP ViT-B/32** on the mushroom dataset using **LoRA** adapters on the **last two vision transformer layers**, with strong data augmentation, adaptive temperature / learning rate, and detailed **error analysis**. This is our *best supervised baseline* on top of CLIP.
  - Key ideas:
    - Freeze base CLIP weights and only train **LoRA adapters** on selected attention projections in layers 10–11 (`q_proj`, `k_proj`, `v_proj`, `out_proj`).
    - Use **enhanced augmentations** on training images (random crops, flips, rotation, affine, color jitter, blur, grayscale, erasing) and CLIP-style normalization.
    - Train with a **difficulty-aware loss** (reweighting by confidence), label smoothing, temperature scaling, and adaptive learning rate schedule with cosine decay + warmup.
    - Evaluate with **Top-1 / Top-5 / balanced accuracy / macro F1** and maintain rich per-class error statistics.
  - Inputs:
    - Dataset:
      - `ROOT_MUSHROOM` = `/zfs/ai4good/datasets/mushroom`
      - `train.csv`, `val.csv` (standard AI4Good mushroom splits).
    - Prompts:
      - `PROMPT_PATH` = `/zfs/ai4good/student/hgupta/enhanced_with_common_names_prompts_clean.json`  
        Per-class enriched prompts (including common names) used to build **text prototypes**.
  - What it does:
    1. **Loads datasets** via `FixedMushroomDataset`:
       - Resolves paths robustly (supports `merged_dataset/...` and original Kaggle-style paths).
       - Optionally performs **balanced sampling** up to `MAX_TRAIN_SAMPLES`.
       - Shares a consistent `class_to_idx` mapping across train/val.
    2. Builds **data loaders**:
       - Train: strong data augmentation (`get_enhanced_train_transform`), shuffled, large batch size (`BATCH_SIZE`).
       - Val: deterministic resize + center crop (`get_enhanced_val_transform`).
    3. Loads **`openai/clip-vit-base-patch32`**, freezes all weights, and wraps it with **LoRA** using:
       - Rank `LORA_R = 128`, `LORA_ALPHA = 256`, `LORA_DROPOUT = 0.1`.
       - Only the specified attention modules are trainable.
    4. Builds **stable text features**:
       - For each class, takes up to 6 prompts from `class_prompts`.
       - Encodes them with CLIP text encoder, normalizes, then **mean-pools per class**.
       - Result: a `text_features` tensor `[num_classes, D]` used as fixed class prototypes.
    5. Training loop:
       - Uses **difficulty-aware loss** (harder examples get higher weight), with:
         - Temperature scheduling `get_adaptive_temperature(epoch, loss)`.
         - Label smoothing (`LABEL_SMOOTHING`).
       - Uses **adaptive LR** `get_adaptive_lr(epoch, batch_idx, total_batches, loss)`:
         - Warmup for first few epochs, then cosine decay with small adjustments based on current loss.
       - Gradient accumulation (`GRAD_ACCUM_STEPS`) and **gradient clipping** with dynamic threshold depending on loss.
       - Tracks loss / LR / temperature / grad norm in `stable_enhanced_log.csv`.
    6. Validation + error analysis:
       - `evaluate_with_error_analysis` computes:
         - Top-1, Top-5, balanced accuracy, macro F1, and validation loss.
       - `ErrorAnalyzer`:
         - Records **per-class misclassifications**, **Top-5 misses**, and **common confusion pairs**.
         - Prints periodic reports and writes a detailed `error_analysis.txt`.
    7. Model selection & early stopping:
       - Tracks best validation **Top-1 accuracy**.
       - Saves best LoRA model to `CHECKPOINT_DIR` (`lora_stable_enhanced`).
       - If Top-1 ≥ 0.85, also saves to `lora_stable_enhanced_85_achieved`.
       - Implements patience-based early stopping (`PATIENCE`).
  - Outputs:
    - Checkpoints:
      - `lora_stable_enhanced/` (best LoRA-tuned CLIP).
      - Optionally `lora_stable_enhanced_85_achieved/` if threshold reached.
    - Logs & reports:
      - `lora_stable_enhanced/stable_enhanced_log.csv`: per-batch + per-epoch metrics and LR/temperature/grad norm.
      - `lora_stable_enhanced/error_analysis.txt`: detailed confusion patterns, Top-5 errors etc.
      - `lora_stable_enhanced/training_summary.txt`: final metrics and training summary.
  - Role in the project:
    - Provides the **main supervised adapter baseline** on top of CLIP.
    - Lets us analyze **which species remain hard** even after adaptation, via rich error analysis.
    - The final numbers feed into the **Experiments** section when comparing:
      - Zero-shot CLIP + prompts vs
      - LoRA-tuned CLIP with enriched prompts and strong augmentation.


- `scripts/_legacy/`  
  - Currently empty. Reserved for deprecated or superseded scripts we may remove later or keep only for archival completeness.


---

## 2. Few-shot scripts: `scripts/few_shots/`

- `scripts/few_shots/clip_utils.py`  
  - Optimized utilities for building CLIP **text embeddings** from prompt templates for each class label (used in few-shot / prompt-ablation experiments).  
  - Supports multiple prompt sets (`v1`, `names`, `ensemble`), optionally augments with common names via `MushroomPromptGenerator`, and returns per-label: raw prompts, per-prompt embeddings, and a mean-pooled label embedding.  
  - Uses `open_clip` with batched tokenization, optional `torch.compile` for `encode_text`, and **disk caching** of embeddings (`.npz` in `/zfs/.../cache_text`) keyed by label + prompts + model.


- `scripts/few_shots/test_few_shots_overall.py`  
  - **Purpose:** Unified driver to evaluate **zero-shot and few-shot** performance of multiple CLIP-style backbones on the mushroom dataset, with **prompt-aware extensions**. It can:
    - Run zero-shot CLIP using text prompts only,
    - Run standard few-shot methods (class prototypes, linear probe),
    - Run **prompt-aware few-shot** methods where image prototypes and/or linear probes are fused with text prompt embeddings (sweeping over α to mix image/text).  
  - **Status:** Actively used. Supports:
    - Multiple backbones with different pretrained weights (`DEFAULT_PRETRAINED`),
    - Cached image and text features (`ensure_features`, `.npz` caching),
    - Optional *direct* evaluation mode that encodes images on-the-fly and compares prompt variants,
    - CSV output (`few_shot_table_all_backbones.csv`) summarizing metrics (Top-1, Top-5, balanced acc, macro F1) across shots, models, and α values.


- `scripts/few_shots/test_few_shots_per_class.py`  
  - **Purpose:** Like `test_few_shots_overall.py`, this script runs **zero-shot and few-shot CLIP evaluations** across multiple backbones, including **prompt-aware** variants (prototype+prompts, linear+prompts). In addition to global metrics, it explicitly tracks **per-class accuracies**:
    - Computes per-class accuracy for each combination of: shot, model type (zero-shot / prototype / linear / +prompts), α, split (val/test), and backbone.
    - Writes a **separate CSV for each class** under `results/per_class/`, so you can inspect which mushroom species benefit from prompts or higher shots, and find outliers.  
  - **Status:** Actively used analysis tool. Produces:
    - A global summary CSV `few_shot_table_all_backbones.csv` (same structure as the overall script),
    - A directory `results/per_class/` with one CSV per species (`class_<id>_<name>.csv`) containing per-class performance across all settings, for detailed error analysis and plots.


- `scripts/few_shots/_legacy/`  
  - Purpose: Folder reserved for deprecated few-shot experiments (currently empty).  

---

## 3. Few-shot hypertuning scripts: `scripts/few_shots/hypertuning/`

- `scripts/few_shots/hypertuning/few_shot_hyper_test.py`  
  - **Purpose:** Large-scale **hyperparameter sweep** script for few-shot CLIP + prompts.  
    - Sweeps over **backbones**, **shots**, **prompt sets** (`ensemble`, `v1`, `names`, `delta`),  
      **alpha** (image–text mixing), **temperature**, **learning rate**, **weight decay**, and **mixing strategy**.  
    - Evaluates **zero-shot**, **prototype**, and **prompt-aware** variants (prototype+prompts, linear+prompts) on val/test, using cached CLIP features from `ensure_features`.  
    - Uses **parallel CPU workers** (`ThreadPoolExecutor`) and a `--cpu-probes` mode to run many small linear-probe trainings in parallel without overloading the GPU.  
    - Can also run a **direct-eval mode** (`--direct-eval`) that encodes images on the fly and tests prompt pooling + few-shot prototypes directly from the CLIP model.  
    - Writes a single JSON file (`few_shot_overall_results.json`) with one record per configuration, including global metrics and an embedded **per-class accuracy dict**.
  - **Status:** Main **tuning/ablation engine** for the project. Used to find the best hyperparameters and prompt settings before training the final model.  
    - Heavy and time-consuming on the full grid; for debugging it supports a `--fast` mode that drastically shrinks the grids and epochs.


- `scripts/few_shots/hypertuning/few_shot_alpha_cast.py`  
  - **Purpose:** Earlier **unified few-shot + zero-shot evaluation** script with a focus on **prompt–image mixing (alpha) sweeps**.  
    - Runs **zero-shot**, **prototype**, and **linear probe** baselines from cached CLIP features.  
    - Adds **prompt-aware few-shot**:
      - `prototype+prompts`: combines image prototypes with text label embeddings using a scalar α.  
      - `linear+prompts`: trains a linear probe on mixed image + prompt features.  
    - Sweeps `alpha` over a grid (`--alpha-grid`) and reports metrics (top-1, top-5, balanced accuracy, macro F1) per `(shot, model, alpha, split, backbone)` in `few_shot_table_all_backbones.csv`.  
    - Also supports an optional **direct-eval mode** (`--direct-eval`) that encodes images on the fly and evaluates prompt pooling + few-shot prototypes directly from the CLIP model.
  - **Status:** **Older / simpler alpha-sweep engine.**  
    - Still useful for **targeted α ablations** and reproducing the original few-shot vs. prompt-aware results with a compact CSV.  
    - For large-scale, fully parallel sweeps and richer grids (temps, LR/WD, prompt sets, mix strategies), it has been **superseded by** `few_shot_hyper_test.py`, which is now the main tuning script.


- `scripts/few_shots/hypertuning/train_best_model.py`  
  - **Purpose:** Final **training + inference** script for the *best mushroom classifier*.  
    - Loads **best hyperparameters** and **per-class α values** (from `best_alpha.json`).  
    - Loads cached **OpenCLIP image features** and **text embeddings** for the chosen backbone (`PE-Core-bigG-14-448`, `meta`).  
    - Builds a **100-shot support set per class**, mixes image features with text label embeddings using **class-specific α**, and trains a **final linear+prompts head** on these mixed features.  
    - Saves a compact PyTorch checkpoint (`final_model.pt`) containing:
      - linear head weights  
      - backbone name + pretrained tag  
      - prompt set  
      - per-class αs  
      - label names  
    - Provides a **prediction pipeline**:
      - Reloads CLIP backbone + linear head  
      - Encodes a new image, mixes its feature with text embeddings (using α per class)  
      - Applies the trained linear head and returns the predicted species label.  
    - Exposes a small **CLI**:
      - `--train` → train and save the final model  
      - `--predict <image_path>` → run single-image inference with the saved model  
  - **Status:** **Final, production-facing script.**  
    - Used to **freeze the chosen configuration** from the hyperparameter sweeps and export a reusable model.  
    - Assumes that:
      - feature caches (`ensure_features`) already exist for the selected backbone, and  
      - `best_alpha.json` has been produced by the alpha-analysis pipeline.  
    - Ready for use in experiments, demos, and downstream tools that need a simple `predict(image_path)` interface.


- `scripts/few_shots/hypertuning/eval_final_model.py`  
  - **Purpose:** Offline **evaluation script** for the final trained mushroom classifier saved in `final_model.pt`.  
    - Reloads:
      - the CLIP backbone + pretrained weights  
      - the final linear head (fixing the `linear.` prefix in state dict keys)  
      - per-class α values and label names  
      - cached text embeddings for the chosen prompt set  
    - Loads cached **val** and **test** image features for the same backbone.  
    - Reconstructs the **mixed features** (image + text) by applying the same per-class α fusion used at training time.  
    - Runs the linear head on these mixed features and computes, **for both validation and test sets**:
      - Top-1 accuracy  
      - Top-5 accuracy  
      - Balanced accuracy  
      - Macro F1  
    - Prints the metrics and saves them to `final_model_eval.json` for reporting / paper tables.  
  - **Status:** **Final evaluation utility.**  
    - Assumes `final_model.pt` and the feature caches already exist.  
    - Meant to be run after `train_best_model.py` to obtain the **official val/test scores** of the selected best model configuration.  


- `scripts/few_shots/hypertuning/few_shot_temp.py`  
  - **Purpose:** Advanced **few-shot + zero-shot sweep script** for CLIP-style backbones with **prompt-aware** extensions.  
    - Reuses the cached CLIP image features (train/val/test) and prompt embeddings.  
    - For each backbone and shot count:
      - Runs **baselines**:
        - Zero-shot (prompt-only, cosine similarity).  
        - Standard image-only prototypes.  
        - Standard linear probe on image features.  
      - Runs **prompt-aware prototypes** and **prompt-aware linear probes**, where:
        - Image prototypes are fused with pooled text prompt embeddings.  
        - Prompt pooling is controlled by a **temperature** (`temp`) over prompt similarities.  
        - Linear probes are trained on a mixed set of image features + text pseudo-examples, weighted by **α**.  
      - Sweeps over:
        - `alpha` (image vs text weight),  
        - `temp` (prompt pooling temperature),  
        - `lr` (learning rate for the linear head),  
        - `wd` (weight decay for the linear head).  
    - Saves:
      - A **flat CSV** (`few_shot_table_all_backbones.csv`) with all runs for quick table-style inspection.  
      - A **nested JSON** (`few_shot_sweep_results.json`) with structured results per backbone, shot, and method (`prototype+prompts`, `linear+prompts`), including metrics and the corresponding hyperparameters.  
    - This is the script you use to **explore the hyperparameter landscape** for prompt-aware few-shot learning (α, temperature, optimizer settings) before picking a final configuration.  
  - **Status:** **Hyperparameter sweep utility – main exploration tool.**  
    - Designed to run potentially heavy sweeps over many combinations of (α, temp, lr, wd) on the cluster.  
    - Output is then mined (e.g., via separate analysis notebook/script) to select the **best configuration** that later feeds into `train_best_model.py`.  


- `scripts/few_shots/hypertuning/few_shot_temp_copy.py`  
  - **Purpose:** Temporary backup of an earlier version of `few_shot_temp.py` used during development/debugging.  
  - **Status:** **Redundant / to be removed or moved to `_legacy/`.**

- `scripts/few_shots/hypertuning/final_few_shots copy.py`  
  - **Purpose:** Old draft of the “final few-shot” script before consolidation into `final_few_shots.py` / `train_best_model.py`.  
  - **Status:** **Deprecated backup – safe to delete or archive under `_legacy/`.**

- `scripts/few_shots/hypertuning/few_shot_hyper_test copy.py`  
  - **Purpose:** First backup copy of `few_shot_hyper_test.py` while refactoring hypertuning logic.  
  - **Status:** **Deprecated – kept only as a snapshot; should be deleted or moved to `_legacy/`.**

- `scripts/few_shots/hypertuning/few_shot_hyper_test copy 2.py`  
  - **Purpose:** Second intermediate variant of `few_shot_hyper_test.py` with experimental changes.  
  - **Status:** **Deprecated experimental file – candidate for removal or `_legacy/`.**

- `scripts/few_shots/hypertuning/few_shot_hyper_test copy 3.py`  
  - **Purpose:** Third backup/experiment around hypertuning, superseded by the cleaned-up main scripts (`few_shot_temp.py`, `few_shot_alpha_cast.py`, etc.).  
  - **Status:** **Obsolete – keep only if you need historical reference; otherwise move to `_legacy/` or delete.**

- `scripts/few_shots/hypertuning/_legacy/`  
  - **Purpose:** Folder reserved for deprecated hypertuning experiments and old prototypes that are no longer part of the main pipeline.  
  - **Status:** **Archival only – no files currently used by the active code.**

